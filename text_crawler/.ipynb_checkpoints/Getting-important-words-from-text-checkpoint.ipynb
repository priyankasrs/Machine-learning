{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://news.mit.edu/2017/auto-tuning-data-science-new-research-streamlines-machine-learning-1219\"\n",
    "html = urlopen(url).read()\n",
    "html[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "div=soup.div\n",
    "text=div.get_text()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using readability for identifying title and Beatiful soup for reading article content\n",
    "from readability.readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "readable_article = Document(html).summary()\n",
    "readable_title = Document(html).title()\n",
    "soup = BeautifulSoup(readable_article,\"lxml\")\n",
    "print ('*** TITLE *** \\n\\\"' + readable_title + '\\\"\\n')\n",
    "print ('*** CONTENT *** \\n\\\"' + soup.text[:500] + '[...]\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding no of occurences of first 30 words\n",
    "tokens = [word for sent in nltk.sent_tokenize(soup.text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "for token in sorted(set(tokens))[:30]:\n",
    "    print (token + ' [' + str(tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big [1]\n",
      "blind [1]\n",
      "both [1]\n",
      "build [1]\n",
      "busi [1]\n",
      "by [6]\n",
      "call [2]\n",
      "came [1]\n",
      "can [8]\n",
      "card [1]\n",
      "choic [1]\n",
      "choos [2]\n",
      "chose [1]\n",
      "close [1]\n",
      "cloud [2]\n",
      "cloud-bas [1]\n",
      "cluster [2]\n",
      "collabor [2]\n",
      "come [1]\n",
      "communiti [1]\n",
      "community-driven [1]\n",
      "compar [3]\n",
      "comparison [1]\n",
      "compet [1]\n",
      "complex [2]\n"
     ]
    }
   ],
   "source": [
    "#inding no of occurences of words 50-75\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "for token in sorted(set(stemmed_tokens))[50:75]:\n",
    "    print (token + ' [' + str(stemmed_tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tremend', 'recent', 'growth', 'of', 'data', 'scienc', '—', 'both', 'as', 'a', 'disciplin', 'and', 'an', 'applic', '—', 'can', 'be', 'attribut', ',', 'in', 'part', ',', 'to', 'it', 'robust', 'problem-solv', 'power', ':', 'it', 'can', 'predict', 'when', 'credit', 'card', 'transact', 'are', 'fraudul', ',', 'help', 'busi', 'owner', 'figur', 'out', 'when', 'to', 'send', 'coupon', 'in', 'order', 'to', 'maxim', 'custom', 'respons', ',', 'or', 'facilit', 'educ', 'intervent', 'by', 'forecast', 'when', 'a', 'student', 'is', 'on', 'the', 'cusp', 'of', 'drop', 'out', '.', 'to', 'get', 'to', 'these', 'data-driven', 'solut', ',', 'though', ',', 'data', 'scientist', 'must', 'shepherd', 'their', 'raw', 'dat']\n",
      "['The', 'tremendous', 'recent', 'growth', 'of', 'data', 'science', '—', 'both', 'a', 'a', 'discipline', 'and', 'an', 'application', '—', 'can', 'be', 'attributed', ',', 'in', 'part', ',', 'to', 'it', 'robust', 'problem-solving', 'power', ':', 'It', 'can', 'predict', 'when', 'credit', 'card', 'transaction', 'are', 'fraudulent', ',', 'help', 'business', 'owner', 'figure', 'out', 'when', 'to', 'send', 'coupon', 'in', 'order', 'to', 'maximize', 'customer', 'response', ',', 'or', 'facilitate', 'educational', 'intervention', 'by', 'forecasting', 'when', 'a', 'student', 'is', 'on', 'the', 'cusp', 'of', 'dropping', 'out', '.', 'To', 'get', 'to', 'these', 'data-driven', 'solution', ',', 'though', ',', 'data', 'scientist', 'must', 'shepherd', 'their', 'raw', 'dat']\n"
     ]
    }
   ],
   "source": [
    "#Comparing output of lemmatizer and stemmer for finding root words\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "temp_sent = soup.text[:500]\n",
    "print ([stemmer.stem(t) for t in nltk.word_tokenize(temp_sent)])\n",
    "print ([lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 59), ('the', 43), ('.', 33), ('a', 32), ('to', 28), ('of', 25), ('and', 24), ('model', 16), ('data', 14), ('in', 13), ('on', 13), ('atm', 11), ('it', 10), ('can', 8), ('scientist', 8), ('system', 8), ('are', 7), ('techniqu', 7), ('differ', 7), ('``', 7), ('for', 7), (\"''\", 7), ('—', 6), ('by', 6), ('solut', 6)]\n"
     ]
    }
   ],
   "source": [
    "#Most commonly occuring words with a lot of stop words\n",
    "fdist = nltk.FreqDist(stemmed_tokens)\n",
    "vocab = list(fdist.most_common(25))\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get already present stop words to ensure that these dont appear in the important words list even though they appear a lot\n",
    "sorted(nltk.corpus.stopwords.words('english'))[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 59), ('.', 33), ('model', 16), ('data', 14), ('atm', 11), ('scientist', 8), ('system', 8), ('techniqu', 7), ('differ', 7), ('``', 7), (\"''\", 7), ('—', 6), ('solut', 6), ('one', 6), ('research', 6), ('human', 6), ('problem', 6), ('work', 6), ('machin', 5), ('select', 5), ('best', 5), ('comput', 5), ('platform', 5), ('say', 5), ('scienc', 4)]\n"
     ]
    }
   ],
   "source": [
    "#The following displays the important words coming from the article\n",
    "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "fdist2 = nltk.FreqDist(stemmed_tokens_no_stop)\n",
    "print (fdist2.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORGANIZATION] IEEE International Conference\n",
      "[ORGANIZATION] MIT\n",
      "[PERSON] Michigan State University\n",
      "[GPE] Models\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Arun Ross\n",
      "[ORGANIZATION] Michigan\n",
      "[ORGANIZATION] University\n",
      "[PERSON] Kalyan Veeramachaneni\n",
      "[ORGANIZATION] MIT’s Laboratory\n",
      "[ORGANIZATION] Information\n",
      "[ORGANIZATION] Decision Systems\n",
      "[ORGANIZATION] LIDS\n",
      "[ORGANIZATION] ATM\n",
      "[GPE] Poor\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Ross\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Veeramachaneni\n",
      "[PERSON] Veeramachaneni\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Veeramachaneni\n"
     ]
    }
   ],
   "source": [
    "#Find and remove proper nouns like names,organizations \n",
    "def extract_entities(text):\n",
    "\tentities = []\n",
    "\tfor sentence in nltk.sent_tokenize(text):\n",
    "\t    chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\t    entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "\treturn entities\n",
    "for entity in extract_entities(soup.text):\n",
    "\n",
    "    print ('[' + entity.label()+ '] ' + ' '.join(c[0] for c in entity.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[O] The\n",
      "[O] tremendous\n",
      "[O] recent\n",
      "[O] growth\n",
      "[O] of\n",
      "[O] data\n",
      "[O] science\n",
      "[O] —\n",
      "[O] bot\n"
     ]
    }
   ],
   "source": [
    "#Usin Stanford NER Library\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "st = StanfordNERTagger('/home/priyanka/Downloads/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               '/home/priyanka/Downloads/stanford-ner/stanford-ner.jar', 'utf-8')\n",
    "\n",
    "for i in st.tag(soup.text[:50].split()):\n",
    "    print ('[' + i[1] + '] ' + i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
