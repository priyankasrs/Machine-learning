{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\n\\n<!DOCTYPE html>\\n<!--[if lt IE 7]>      <html class=\"no-js lt-ie9 lt-ie8 lt-ie7\"> <![endif]-->\\n<!--[if IE 7]>         <html class=\"no-js lt-ie9 lt-ie8\"> <![endif]-->\\n<!--[if IE 8]>         <html class=\"no-js lt-ie9\"> <![endif]-->\\n<!--[if gt IE 8]><!--> <html class=\"no-js\"> <!--<![endif]-->\\n<head>\\n    <meta charset=\"utf-8\">\\n    <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\">\\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1, maximum-scale=1, user-scalable=0\">\\n    <meta'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://news.mit.edu/2017/auto-tuning-data-science-new-research-streamlines-machine-learning-1219\"\n",
    "html = urlopen(url).read()\n",
    "html[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\n\\n\\n                        Massachusetts Institute of Technology\\n                    \\n\\n\\n\\n\\nNews\\nVideo\\nSocial\\n\\nFollow MIT\\n\\nMIT News RSS\\nFollow MIT on Twitter\\nFollow MIT on Facebook\\nFollow MIT on Google+\\nFollow MIT on Instagram\\nFollow MIT on Flickr\\nFollow MIT on YouTube\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nMIT News Office\\n\\n\\n\\n\\n\\n\\nBrowse\\nor\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBrowse\\n\\n\\nMost Popular\\n\\n\\n\\n\\n\\n \\n\\n\\n \\n\\nHUBweek Policy Hackathon delivers local solutions \\n\\n\\nStudying the hotbed of horizontal gene transfers \\n\\n\\nProfessor Emeritus Sylvain Brombe'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "div=soup.div\n",
    "text=div.get_text()\n",
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** TITLE *** \n",
      "\"Auto-tuning data science: New research streamlines machine learning | MIT News\"\n",
      "\n",
      "*** CONTENT *** \n",
      "\"The tremendous recent growth of data science — both as a discipline and an application — can be attributed, in part, to its robust problem-solving power: It can predict when credit card transactions are fraudulent, help business owners figure out when to send coupons in order to maximize customer response, or facilitate educational interventions by forecasting when a student is on the cusp of dropping out.\n",
      "To get to these data-driven solutions, though, data scientists must shepherd their raw dat[...]\"\n"
     ]
    }
   ],
   "source": [
    "#Using readability for identifying title and Beatiful soup for reading article content\n",
    "from readability.readability import Document\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "readable_article = Document(html).summary()\n",
    "readable_title = Document(html).title()\n",
    "soup = BeautifulSoup(readable_article,\"lxml\")\n",
    "print ('*** TITLE *** \\n\\\"' + readable_title + '\\\"\\n')\n",
    "print ('*** CONTENT *** \\n\\\"' + soup.text[:500] + '[...]\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' [1]\n",
      "'' [7]\n",
      "'s [2]\n",
      "'what-if [1]\n",
      "( [3]\n",
      ") [3]\n",
      ", [59]\n",
      ". [33]\n",
      "100 [1]\n",
      "30 [1]\n",
      "47 [1]\n",
      ": [3]\n",
      "? [1]\n",
      "A [2]\n",
      "ATM [11]\n",
      "And [1]\n",
      "Arun [1]\n",
      "Auto-ML [4]\n",
      "Auto-Tuned [1]\n",
      "Big [1]\n",
      "By [1]\n",
      "Competing [1]\n",
      "Conference [1]\n",
      "Data [1]\n",
      "Decision [1]\n",
      "Empowering [1]\n",
      "IEEE [1]\n",
      "If [1]\n",
      "In [3]\n",
      "Information [1]\n"
     ]
    }
   ],
   "source": [
    "#Finding no of occurences of first 30 words\n",
    "tokens = [word for sent in nltk.sent_tokenize(soup.text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "for token in sorted(set(tokens))[:30]:\n",
    "    print (token + ' [' + str(tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big [1]\n",
      "blind [1]\n",
      "both [1]\n",
      "build [1]\n",
      "busi [1]\n",
      "by [6]\n",
      "call [2]\n",
      "came [1]\n",
      "can [8]\n",
      "card [1]\n",
      "choic [1]\n",
      "choos [2]\n",
      "chose [1]\n",
      "close [1]\n",
      "cloud [2]\n",
      "cloud-bas [1]\n",
      "cluster [2]\n",
      "collabor [2]\n",
      "come [1]\n",
      "communiti [1]\n",
      "community-driven [1]\n",
      "compar [3]\n",
      "comparison [1]\n",
      "compet [1]\n",
      "complex [2]\n"
     ]
    }
   ],
   "source": [
    "#finding no of occurences of words 50-75\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "for token in sorted(set(stemmed_tokens))[50:75]:\n",
    "    print (token + ' [' + str(stemmed_tokens.count(token)) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'tremend', 'recent', 'growth', 'of', 'data', 'scienc', '—', 'both', 'as', 'a', 'disciplin', 'and', 'an', 'applic', '—', 'can', 'be', 'attribut', ',', 'in', 'part', ',', 'to', 'it', 'robust', 'problem-solv', 'power', ':', 'it', 'can', 'predict', 'when', 'credit', 'card', 'transact', 'are', 'fraudul', ',', 'help', 'busi', 'owner', 'figur', 'out', 'when', 'to', 'send', 'coupon', 'in', 'order', 'to', 'maxim', 'custom', 'respons', ',', 'or', 'facilit', 'educ', 'intervent', 'by', 'forecast', 'when', 'a', 'student', 'is', 'on', 'the', 'cusp', 'of', 'drop', 'out', '.', 'to', 'get', 'to', 'these', 'data-driven', 'solut', ',', 'though', ',', 'data', 'scientist', 'must', 'shepherd', 'their', 'raw', 'dat']\n",
      "['The', 'tremendous', 'recent', 'growth', 'of', 'data', 'science', '—', 'both', 'a', 'a', 'discipline', 'and', 'an', 'application', '—', 'can', 'be', 'attributed', ',', 'in', 'part', ',', 'to', 'it', 'robust', 'problem-solving', 'power', ':', 'It', 'can', 'predict', 'when', 'credit', 'card', 'transaction', 'are', 'fraudulent', ',', 'help', 'business', 'owner', 'figure', 'out', 'when', 'to', 'send', 'coupon', 'in', 'order', 'to', 'maximize', 'customer', 'response', ',', 'or', 'facilitate', 'educational', 'intervention', 'by', 'forecasting', 'when', 'a', 'student', 'is', 'on', 'the', 'cusp', 'of', 'dropping', 'out', '.', 'To', 'get', 'to', 'these', 'data-driven', 'solution', ',', 'though', ',', 'data', 'scientist', 'must', 'shepherd', 'their', 'raw', 'dat']\n"
     ]
    }
   ],
   "source": [
    "#Comparing output of lemmatizer and stemmer for finding root words\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "temp_sent = soup.text[:500]\n",
    "print ([stemmer.stem(t) for t in nltk.word_tokenize(temp_sent)])\n",
    "print ([lemmatizer.lemmatize(t) for t in nltk.word_tokenize(temp_sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 59), ('the', 43), ('.', 33), ('a', 32), ('to', 28), ('of', 25), ('and', 24), ('model', 16), ('data', 14), ('in', 13), ('on', 13), ('it', 11), ('atm', 11), ('can', 8), ('scientist', 8), ('system', 8), ('are', 7), ('techniqu', 7), ('differ', 7), ('``', 7), ('for', 7), (\"''\", 7), ('—', 6), ('by', 6), ('solut', 6)]\n"
     ]
    }
   ],
   "source": [
    "#Most commonly occuring words with a lot of stop words\n",
    "fdist = nltk.FreqDist(stemmed_tokens)\n",
    "vocab = list(fdist.most_common(25))\n",
    "print (vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get already present stop words to ensure that these dont appear in the important words list even though they appear a lot\n",
    "sorted(nltk.corpus.stopwords.words('english'))[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 59), ('.', 33), ('model', 16), ('data', 14), ('atm', 11), ('scientist', 8), ('system', 8), ('techniqu', 7), ('differ', 7), ('``', 7), (\"''\", 7), ('—', 6), ('solut', 6), ('one', 6), ('best', 6), ('research', 6), ('human', 6), ('problem', 6), ('work', 6), ('machin', 5), ('select', 5), ('comput', 5), ('platform', 5), ('say', 5), ('scienc', 4)]\n"
     ]
    }
   ],
   "source": [
    "#The following displays the important words coming from the article\n",
    "stemmed_tokens_no_stop = [stemmer.stem(t) for t in stemmed_tokens if t not in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "fdist2 = nltk.FreqDist(stemmed_tokens_no_stop)\n",
    "print (fdist2.most_common(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ORGANIZATION] IEEE International Conference\n",
      "[ORGANIZATION] MIT\n",
      "[PERSON] Michigan State University\n",
      "[GPE] Models\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Arun Ross\n",
      "[ORGANIZATION] Michigan\n",
      "[ORGANIZATION] University\n",
      "[PERSON] Kalyan Veeramachaneni\n",
      "[ORGANIZATION] MIT\n",
      "[ORGANIZATION] Laboratory\n",
      "[ORGANIZATION] Information\n",
      "[ORGANIZATION] Decision Systems\n",
      "[ORGANIZATION] LIDS\n",
      "[ORGANIZATION] ATM\n",
      "[GPE] Poor\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Ross\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Veeramachaneni\n",
      "[PERSON] Veeramachaneni\n",
      "[ORGANIZATION] ATM\n",
      "[PERSON] Veeramachaneni\n"
     ]
    }
   ],
   "source": [
    "#Find and remove proper nouns like names,organizations \n",
    "def extract_entities(text):\n",
    "\tentities = []\n",
    "\tfor sentence in nltk.sent_tokenize(text):\n",
    "\t    chunks = nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sentence)))\n",
    "\t    entities.extend([chunk for chunk in chunks if hasattr(chunk, 'label')])\n",
    "\treturn entities\n",
    "for entity in extract_entities(soup.text):\n",
    "\n",
    "    print ('[' + entity.label()+ '] ' + ' '.join(c[0] for c in entity.leaves()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-ner.jar jar file at /home/priyanka/Downloads/stanford-ner/stanford-ner.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-006b2bc7ba14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m st = StanfordNERTagger('/home/priyanka/Downloads/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n\u001b[0;32m----> 5\u001b[0;31m                '/home/priyanka/Downloads/stanford-ner/stanford-ner.jar', 'utf-8')\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStanfordNERTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/tag/stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_filename, path_to_jar, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_JAR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 verbose=verbose)\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         self._stanford_model = find_file(model_filename,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    719\u001b[0m         searchpath=(), url=None, verbose=False, is_regex=False):\n\u001b[1;32m    720\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 721\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/nltk/__init__.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 637\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-ner.jar jar file at /home/priyanka/Downloads/stanford-ner/stanford-ner.jar"
     ]
    }
   ],
   "source": [
    "#Usin Stanford NER Library\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "st = StanfordNERTagger('/home/priyanka/Downloads/stanford-ner/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
    "               '/home/priyanka/Downloads/stanford-ner/stanford-ner.jar', 'utf-8')\n",
    "\n",
    "for i in st.tag(soup.text[:50].split()):\n",
    "    print ('[' + i[1] + '] ' + i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
